---
title: "Fine-tuning Machine Learning Models for Predicting Functional Groups from IR Spectra"
date: 20/06/2025
---

import { Sidenote, Tldr } from "@/components/blog"

<span>{meta.date}</span>
<h1>{meta.title}</h1>
<h2 className="text-xl">Enhancing IR spectral analysis accuracy through fine-tuned ML models</h2>

***Note:** this project was a joint effort as part of a CBL at the Eindhoven University of Technology. Credit to my group-members Stefan Avram, Rick Cruts, Bartosz Janik, [Tim Nazarian](https://www.linkedin.com/in/tim-nazarian) and Gerardo Herrera Herrero.*

<Tldr>
  <p className="mt-2">
    We fine-tuned a machine learning model to analyze chemical mixtures with IR spectroscopy, as existing models only handle pure compounds. Our process involved reproducing a state-of-the-art model, scraping and processing 60,000+ spectra, and then fine-tuning for binary mixtures.
  </p>
</Tldr>


# Research Goal(s)

The idea of the project was to apply machine learning and predictive analysis techniques to Infrared Spectroscopy (IR Spectroscopy) analysis to improve the sustainability of reactors. Though, immediately it was clear that a bigger issue was at hand. *How could we work on sustainability when the models currently found cannot even identify functional groups in mixtures?*

Hence, the idea to research how to adapt current IR spectroscopy prediction models to mixtures arose. We decided to focus on two main goals:

1. Can models trained on pure compound spectra accurately identify functional groups in binary mixtures?
1. If accuracy drops with mixtures, can we improve performance through artificial data augmentation techniques, spectral combination algorithms and targeted fine-tuning approaches?

# Reproducing an existing model

From all the research we made, it was clear that there had been many attempts in the past, by scientific researchers, to create models capable of identifying functional groups (FGs) in IR spectra, though many of them were out of range. Approaches such as using Quantum Simulation, and even transformers were just out of the realm of what was achievable in the 2 months we had to complete this project. Hence, we settled for reproducing the model by [Jung et. al.](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc05892h) published in 2023.

The code was, to put it simply... *ugly*. There was no documentation, missing files but, worse of all, wrong data. Nonetheless, we began fixing it. One step at a time, on file at a time, we began implementing new functions to *glue* everything together.

<Sidenote>
  <strong>Sidenote:</strong> I never truly understood the impact of long-term tech debt and how poorly documented code wastes time for future teams—until this project. From now on, I’m committed to writing clear, well-justified code. Definitely a valuable lesson learned!
</Sidenote>

## Web Scraping

As any good ML project, it all starts with the data, and **lots of it**. We scoured the web for data, and ended up, as the original model did, on SDBS and NIST. We utilized Jung et. al.'s list of NIST and SDBS id's to be used for training and testing of the model.

I developed 2 web scrapers, one for each dataset, which allowed us to download **over 60,000 spectra** in under 2 days, a pretty decent result considering the limitations for scrapers imposed by the websites. I relied on *Selenium* to build the scrapers, as it was the technology which on the one end I was most familiar with and on the other hand extremely well documented.

To run the projects non-stop I set up my desktop computer as a server, enabling SSH access to it over the network and starting multiple headless browser sessions running simultaneously. 

## Data processing

Most of the IR spectra returned were in `.gif` or .`jdx`(JCAMP-DX, a typical file format for IR spectroscopy results). The .jdx were not the issue, as they already contain the values for transmittance at every wavelength. The issue were the `.gif`'s downloaded from SDBS. Not only were they low resolution, but even worse, they were of different sizes. 

Clearly we had to find a way to parse these images of spectra into actual values. Jung et al. in their repo propose a script which uses fixed values for countour detection, handled by OpenCV. Clearly we had to adapt this for our task at hand. Thus, I proceeded going case by case, attempting to see where the fixed-values broke and ended up re-writing most of the scripts (and yes, I did document the changes I made).

While tedious, this process turned out to be a good refresher on my previous encounters with OpenCV, as I had used it in the past to attempt to make a SLAM (Simultaneous Location and Mapping) inspired by a Geohot video. This was tough, with a lot of battles with the algorithm picking up an axis as if it were part of the actual spectra and so on. Finally, after a good 6-8 hours, I had a working script and it was time to parse all 60k+ spectra, converting both JCAMP-DX files and the *nightmareish* GIF images into a unified format.

